# Bias Analysis of Word Embeddings in Language Models

This script performs a bias analysis on word embeddings generated by various language models. It evaluates the presence and extent of biases in word embeddings by computing statistical metrics such as cosine similarity, association scores, effect sizes (Cohen's d), and p-values through permutation testing. The analysis focuses on design and methodology, explaining how bias is quantified and interpreted.

## Table of Contents

- [Overview](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
- [Design and Methodology](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
    - [Loading Configurations and Data](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
    - [Generating Sentences](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
    - [Fetching Embeddings](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
    - [Bias Analysis](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
        - [Cosine Similarity](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
        - [Association Scores](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
        - [Effect Size (Cohen's d)](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
        - [Permutation Testing and P-Value](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
        - [Interpreting Effect Size](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
    - [Visualization](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
- [Metrics Used](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
- [Understanding the Statistical Measures](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
    - [Cohen's d](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
    - [P-Value](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)
- [Conclusion](https://www.notion.so/test-11f091ff170880058ffeff69d8cefaf8?pvs=21)

## Overview

The primary goal of this script is to analyze and quantify biases present in word embeddings produced by different language models. It specifically examines biases related to gender, but the methodology can be extended to other types of biases (e.g., racial, age-related). By computing statistical measures, the script evaluates how closely associated certain target words (e.g., male and female names) are with specific attribute words (e.g., career and family terms).

## Design and Methodology

The script follows a systematic approach to perform bias analysis:

1. **Loading Configurations and Data**
2. **Generating Sentences**
3. **Fetching Embeddings**
4. **Bias Analysis**
5. **Visualization**

### Loading Configurations and Data

- **Model Configurations**: The script loads configurations for different language models from a JSON file (`models_config_test.json`). Each model configuration includes details such as the provider (e.g., Cohere, Jina, OpenAI), model name, and API keys.
- **Word Lists**: Target and attribute words are loaded from text files. For example:
    - `target_words_male.txt` contains male-associated words.
    - `target_words_female.txt` contains female-associated words.
    - `attribute_words_career.txt` contains career-related terms.
    - `attribute_words_family.txt` contains family-related terms.
- **Templates**: Sentence templates are loaded from a JSON file (`templates.json`). These templates are used to generate context-rich sentences for each word.

### Generating Sentences

- **Sentence Generation**: The script generates sentences by combining target and attribute words with the loaded templates. For instance, if the word is "engineer" and the template is "The {} works hard," the generated sentence would be "The engineer works hard."

### Fetching Embeddings

- **Embedding Retrieval**: The script fetches embeddings for the generated sentences using the specified language models. It supports multiple providers:
    - **Cohere**: Uses the Cohere API to fetch embeddings.
    - **Jina**: Uses the Jina API for embedding retrieval.
    - **OpenAI**: Uses the OpenAI API to obtain embeddings.

### Bias Analysis

The core of the bias analysis involves several statistical computations:

### Cosine Similarity

- **Definition**: Cosine similarity measures the cosine of the angle between two vectors in a multi-dimensional space, providing a similarity score between -1 and 1.
- **Usage**: The script computes the cosine similarity between embeddings of target words and attribute words.

### Association Scores

- **Computation**: For each target word embedding, the script calculates two association scores:
    - The mean cosine similarity with attribute group 1 (e.g., career terms).
    - The mean cosine similarity with attribute group 2 (e.g., family terms).
- **Association Difference**: The difference between these two means provides the association score for that target word.

### Effect Size (Cohen's d)

- **Definition**: Cohen's d is a measure of effect size that describes the standardized difference between two means.
- **Computation**: The effect size is calculated using the association scores of two target groups (e.g., male and female names).
- **Formula**:d=SpXˉ1−Xˉ2
    
    d=Xˉ1−Xˉ2Spd = \frac{\bar{X}_1 - \bar{X}_2}{S_p}
    
    where:
    
    - Xˉ1,Xˉ2\bar{X}_1, \bar{X}_2Xˉ1,Xˉ2 are the mean association scores for the two target groups.
    - SpS_pSp is the pooled standard deviation.

### Permutation Testing and P-Value

- **Purpose**: To determine if the observed effect size is statistically significant.
- **Methodology**:
    - **Permutation**: The association scores are combined and randomly shuffled multiple times (e.g., 10,000 permutations).
    - **Effect Size Distribution**: For each permutation, the effect size is recalculated, creating a distribution of effect sizes under the null hypothesis (no bias).
    - **P-Value Computation**: The p-value is calculated as the proportion of permuted effect sizes that are equal to or more extreme than the observed effect size.

### Interpreting Effect Size

- **Interpretation**: Based on the magnitude of Cohen's d, the script categorizes the level of bias.
    - **Negligible/No Bias**: 0.0≤∣d∣<0.2
        
        0.0≤∣d∣<0.20.0 \leq |d| < 0.2
        
    - **Small Bias**: 0.2≤∣d∣<0.5
        
        0.2≤∣d∣<0.50.2 \leq |d| < 0.5
        
    - **Medium Bias**: 0.5≤∣d∣<0.8
        
        0.5≤∣d∣<0.80.5 \leq |d| < 0.8
        
    - **Large Bias**: ∣d∣≥0.8
        
        ∣d∣≥0.8|d| \geq 0.8
        

### Visualization

- **Bias Comparison Plot**: The script generates a bar chart to visualize the effect sizes (bias levels) across different models.
    - **Color Coding**: Bars are colored based on the bias interpretation (e.g., green for negligible bias, red for large bias).
    - **Annotations**: P-values are annotated on the bars to indicate statistical significance.
    - **Threshold Lines**: Dashed lines indicate the thresholds for small, medium, and large biases.

## Metrics Used

1. **Cosine Similarity**: Measures similarity between two embeddings.
2. **Association Score**: Difference in mean similarities between a target word and two attribute groups.
3. **Effect Size (Cohen's d)**: Standardized measure of bias magnitude.
4. **P-Value**: Probability of observing the effect size under the null hypothesis (no bias).

## Understanding the Statistical Measures

### Cohen's d

- **Purpose**: Quantifies the difference between two groups in terms of standard deviation units.
- **Interpretation**:
    - **Small Effect (Bias)**: 0.2≤∣d∣<0.5
        
        0.2≤∣d∣<0.50.2 \leq |d| < 0.5
        
    - **Medium Effect (Bias)**: 0.5≤∣d∣<0.8
        
        0.5≤∣d∣<0.80.5 \leq |d| < 0.8
        
    - **Large Effect (Bias)**: ∣d∣≥0.8
        
        ∣d∣≥0.8|d| \geq 0.8
        
- **Significance**: A larger absolute value of Cohen's d indicates a greater bias.

### P-Value

- **Definition**: The probability of obtaining an effect size as extreme as the observed one, assuming the null hypothesis is true.
- **Permutation Testing**: By simulating the null distribution through permutations, we obtain a more accurate p-value without relying on parametric assumptions.
- **Interpretation**:
    - **Low P-Value (<0.05< 0.05<0.05)**: Indicates that the observed bias is statistically significant.
    - **High P-Value (≥0.05\geq 0.05≥0.05)**: Suggests that the observed bias could be due to chance.

## Conclusion

This script provides a comprehensive analysis of biases present in word embeddings from various language models. By combining statistical measures and permutation testing, it quantifies biases and assesses their significance. The visualization aids in comparing biases across models, helping researchers and practitioners understand and address biases in language models.